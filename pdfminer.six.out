7
1
0
2
 
n
a
J
 
0
3
 
 
]

G
L
.
s
c
[
 
 
9
v
0
8
9
6
.
2
1
4
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2015ADAM:AMETHODFORSTOCHASTICOPTIMIZATIONDiederikP.Kingma*UniversityofAmsterdam,OpenAIdpkingma@openai.comJimmyLeiBa∗UniversityofTorontojimmy@psi.utoronto.caABSTRACTWeintroduceAdam,analgorithmforﬁrst-ordergradient-basedoptimizationofstochasticobjectivefunctions,basedonadaptiveestimatesoflower-ordermo-ments.Themethodisstraightforwardtoimplement,iscomputationallyefﬁcient,haslittlememoryrequirements,isinvarianttodiagonalrescalingofthegradients,andiswellsuitedforproblemsthatarelargeintermsofdataand/orparameters.Themethodisalsoappropriatefornon-stationaryobjectivesandproblemswithverynoisyand/orsparsegradients.Thehyper-parametershaveintuitiveinterpre-tationsandtypicallyrequirelittletuning.Someconnectionstorelatedalgorithms,onwhichAdamwasinspired,arediscussed.Wealsoanalyzethetheoreticalcon-vergencepropertiesofthealgorithmandprovidearegretboundontheconver-genceratethatiscomparabletothebestknownresultsundertheonlineconvexoptimizationframework.EmpiricalresultsdemonstratethatAdamworkswellinpracticeandcomparesfavorablytootherstochasticoptimizationmethods.Finally,wediscussAdaMax,avariantofAdambasedontheinﬁnitynorm.1INTRODUCTIONStochasticgradient-basedoptimizationisofcorepracticalimportanceinmanyﬁeldsofscienceandengineering.Manyproblemsintheseﬁeldscanbecastastheoptimizationofsomescalarparameter-izedobjectivefunctionrequiringmaximizationorminimizationwithrespecttoitsparameters.Ifthefunctionisdifferentiablew.r.t.itsparameters,gradientdescentisarelativelyefﬁcientoptimizationmethod,sincethecomputationofﬁrst-orderpartialderivativesw.r.t.alltheparametersisofthesamecomputationalcomplexityasjustevaluatingthefunction.Often,objectivefunctionsarestochastic.Forexample,manyobjectivefunctionsarecomposedofasumofsubfunctionsevaluatedatdifferentsubsamplesofdata;inthiscaseoptimizationcanbemademoreefﬁcientbytakinggradientstepsw.r.t.individualsubfunctions,i.e.stochasticgradientdescent(SGD)orascent.SGDproveditselfasanefﬁcientandeffectiveoptimizationmethodthatwascentralinmanymachinelearningsuccessstories,suchasrecentadvancesindeeplearning(Dengetal.,2013;Krizhevskyetal.,2012;Hinton&Salakhutdinov,2006;Hintonetal.,2012a;Gravesetal.,2013).Objectivesmayalsohaveothersourcesofnoisethandatasubsampling,suchasdropout(Hintonetal.,2012b)regularization.Forallsuchnoisyobjectives,efﬁcientstochasticoptimizationtechniquesarerequired.Thefocusofthispaperisontheoptimizationofstochasticobjectiveswithhigh-dimensionalparametersspaces.Inthesecases,higher-orderoptimizationmethodsareill-suited,anddiscussioninthispaperwillberestrictedtoﬁrst-ordermethods.WeproposeAdam,amethodforefﬁcientstochasticoptimizationthatonlyrequiresﬁrst-ordergra-dientswithlittlememoryrequirement.Themethodcomputesindividualadaptivelearningratesfordifferentparametersfromestimatesofﬁrstandsecondmomentsofthegradients;thenameAdamisderivedfromadaptivemomentestimation.Ourmethodisdesignedtocombinetheadvantagesoftworecentlypopularmethods:AdaGrad(Duchietal.,2011),whichworkswellwithsparsegra-dients,andRMSProp(Tieleman&Hinton,2012),whichworkswellinon-lineandnon-stationarysettings;importantconnectionstotheseandotherstochasticoptimizationmethodsareclariﬁedinsection5.SomeofAdam’sadvantagesarethatthemagnitudesofparameterupdatesareinvarianttorescalingofthegradient,itsstepsizesareapproximatelyboundedbythestepsizehyperparameter,itdoesnotrequireastationaryobjective,itworkswithsparsegradients,anditnaturallyperformsaformofstepsizeannealing.∗Equalcontribution.AuthororderingdeterminedbycoinﬂipoveraGoogleHangout.1PublishedasaconferencepaperatICLR2015Algorithm1:Adam,ourproposedalgorithmforstochasticoptimization.Seesection2fordetails,andforaslightlymoreefﬁcient(butlessclear)orderofcomputation.g2tindicatestheelementwisesquaregt(cid:12)gt.Gooddefaultsettingsforthetestedmachinelearningproblemsareα=0.001,β1=0.9,β2=0.999and(cid:15)=10−8.Alloperationsonvectorsareelement-wise.Withβt1andβt2wedenoteβ1andβ2tothepowert.Require:α:StepsizeRequire:β1,β2∈[0,1):ExponentialdecayratesforthemomentestimatesRequire:f(θ):StochasticobjectivefunctionwithparametersθRequire:θ0:Initialparametervectorm0←0(Initialize1stmomentvector)v0←0(Initialize2ndmomentvector)t←0(Initializetimestep)whileθtnotconvergeddot←t+1gt←∇θft(θt−1)(Getgradientsw.r.t.stochasticobjectiveattimestept)mt←β1·mt−1+(1−β1)·gt(Updatebiasedﬁrstmomentestimate)vt←β2·vt−1+(1−β2)·g2t(Updatebiasedsecondrawmomentestimate)bmt←mt/(1−βt1)(Computebias-correctedﬁrstmomentestimate)bvt←vt/(1−βt2)(Computebias-correctedsecondrawmomentestimate)θt←θt−1−α·bmt/(√bvt+(cid:15))(Updateparameters)endwhilereturnθt(Resultingparameters)Insection2wedescribethealgorithmandthepropertiesofitsupdaterule.Section3explainsourinitializationbiascorrectiontechnique,andsection4providesatheoreticalanalysisofAdam’sconvergenceinonlineconvexprogramming.Empirically,ourmethodconsistentlyoutperformsothermethodsforavarietyofmodelsanddatasets,asshowninsection6.Overall,weshowthatAdamisaversatilealgorithmthatscalestolarge-scalehigh-dimensionalmachinelearningproblems.2ALGORITHMSeealgorithm1forpseudo-codeofourproposedalgorithmAdam.Letf(θ)beanoisyobjec-tivefunction:astochasticscalarfunctionthatisdifferentiablew.r.t.parametersθ.Wearein-terestedinminimizingtheexpectedvalueofthisfunction,E[f(θ)]w.r.t.itsparametersθ.Withf1(θ),...,,fT(θ)wedenotetherealisationsofthestochasticfunctionatsubsequenttimesteps1,...,T.Thestochasticitymightcomefromtheevaluationatrandomsubsamples(minibatches)ofdatapoints,orarisefrominherentfunctionnoise.Withgt=∇θft(θ)wedenotethegradient,i.e.thevectorofpartialderivativesofft,w.r.tθevaluatedattimestept.Thealgorithmupdatesexponentialmovingaveragesofthegradient(mt)andthesquaredgradient(vt)wherethehyper-parametersβ1,β2∈[0,1)controltheexponentialdecayratesofthesemovingaverages.Themovingaveragesthemselvesareestimatesofthe1stmoment(themean)andthe2ndrawmoment(theuncenteredvariance)ofthegradient.However,thesemovingaveragesareinitializedas(vectorsof)0’s,leadingtomomentestimatesthatarebiasedtowardszero,especiallyduringtheinitialtimesteps,andespeciallywhenthedecayratesaresmall(i.e.theβsarecloseto1).Thegoodnewsisthatthisinitializationbiascanbeeasilycounteracted,resultinginbias-correctedestimatesbmtandbvt.Seesection3formoredetails.Notethattheefﬁciencyofalgorithm1can,attheexpenseofclarity,beimproveduponbychangingtheorderofcomputation,e.g.byreplacingthelastthreelinesintheloopwiththefollowinglines:αt=α·p1−βt2/(1−βt1)andθt←θt−1−αt·mt/(√vt+ˆ(cid:15)).2.1ADAM’SUPDATERULEAnimportantpropertyofAdam’supdateruleisitscarefulchoiceofstepsizes.Assuming(cid:15)=0,theeffectivesteptakeninparameterspaceattimesteptis∆t=α·bmt/√bvt.Theeffectivestepsizehastwoupperbounds:|∆t|≤α·(1−β1)/√1−β2inthecase(1−β1)>√1−β2,and|∆t|≤α2PublishedasaconferencepaperatICLR2015otherwise.Theﬁrstcaseonlyhappensinthemostseverecaseofsparsity:whenagradienthasbeenzeroatalltimestepsexceptatthecurrenttimestep.Forlesssparsecases,theeffectivestepsizewillbesmaller.When(1−β1)=√1−β2wehavethat|bmt/√bvt|<1therefore|∆t|<α.Inmorecommonscenarios,wewillhavethatbmt/√bvt≈±1since|E[g]/pE[g2]|≤1.Theeffectivemagnitudeofthestepstakeninparameterspaceateachtimestepareapproximatelyboundedbythestepsizesettingα,i.e.,|∆t|/α.Thiscanbeunderstoodasestablishingatrustregionaroundthecurrentparametervalue,beyondwhichthecurrentgradientestimatedoesnotprovidesufﬁcientinformation.Thistypicallymakesitrelativelyeasytoknowtherightscaleofαinadvance.Formanymachinelearningmodels,forinstance,weoftenknowinadvancethatgoodoptimaarewithhighprobabilitywithinsomesetregioninparameterspace;itisnotuncommon,forexample,tohaveapriordistributionovertheparameters.Sinceαsets(anupperboundof)themagnitudeofstepsinparameterspace,wecanoftendeducetherightorderofmagnitudeofαsuchthatoptimacanbereachedfromθ0withinsomenumberofiterations.Withaslightabuseofterminology,wewillcalltheratiobmt/√bvtthesignal-to-noiseratio(SNR).WithasmallerSNRtheeffectivestepsize∆twillbeclosertozero.Thisisadesirableproperty,sinceasmallerSNRmeansthatthereisgreateruncertaintyaboutwhetherthedirectionofbmtcorrespondstothedirectionofthetruegradient.Forexample,theSNRvaluetypicallybecomescloserto0towardsanoptimum,leadingtosmallereffectivestepsinparameterspace:aformofautomaticannealing.Theeffectivestepsize∆tisalsoinvarianttothescaleofthegradients;rescalingthegradientsgwithfactorcwillscalebmtwithafactorcandbvtwithafactorc2,whichcancelout:(c·bmt)/(√c2·bvt)=bmt/√bvt.3INITIALIZATIONBIASCORRECTIONAsexplainedinsection2,Adamutilizesinitializationbiascorrectionterms.Wewillherederivethetermforthesecondmomentestimate;thederivationfortheﬁrstmomentestimateiscompletelyanalogous.Letgbethegradientofthestochasticobjectivef,andwewishtoestimateitssecondrawmoment(uncenteredvariance)usinganexponentialmovingaverageofthesquaredgradient,withdecayrateβ2.Letg1,...,gTbethegradientsatsubsequenttimesteps,eachadrawfromanunderlyinggradientdistributiongt∼p(gt).Letusinitializetheexponentialmovingaverageasv0=0(avectorofzeros).Firstnotethattheupdateattimesteptoftheexponentialmovingaveragevt=β2·vt−1+(1−β2)·g2t(whereg2tindicatestheelementwisesquaregt(cid:12)gt)canbewrittenasafunctionofthegradientsatallprevioustimesteps:vt=(1−β2)tXi=1βt−i2·g2i(1)WewishtoknowhowE[vt],theexpectedvalueoftheexponentialmovingaverageattimestept,relatestothetruesecondmomentE[g2t],sowecancorrectforthediscrepancybetweenthetwo.Takingexpectationsoftheleft-handandright-handsidesofeq.(1):E[vt]=E"(1−β2)tXi=1βt−i2·g2i#(2)=E[g2t]·(1−β2)tXi=1βt−i2+ζ(3)=E[g2t]·(1−βt2)+ζ(4)whereζ=0ifthetruesecondmomentE[g2i]isstationary;otherwiseζcanbekeptsmallsincetheexponentialdecayrateβ1can(andshould)bechosensuchthattheexponentialmovingaverageassignssmallweightstogradientstoofarinthepast.Whatisleftistheterm(1−βt2)whichiscausedbyinitializingtherunningaveragewithzeros.Inalgorithm1wethereforedividebythistermtocorrecttheinitializationbias.Incaseofsparsegradients,forareliableestimateofthesecondmomentoneneedstoaverageovermanygradientsbychosingasmallvalueofβ2;howeveritisexactlythiscaseofsmallβ2wherealackofinitialisationbiascorrectionwouldleadtoinitialstepsthataremuchlarger.3PublishedasaconferencepaperatICLR20154CONVERGENCEANALYSISWeanalyzetheconvergenceofAdamusingtheonlinelearningframeworkproposedin(Zinkevich,2003).Givenanarbitrary,unknownsequenceofconvexcostfunctionsf1(θ),f2(θ),...,fT(θ).Ateachtimet,ourgoalistopredicttheparameterθtandevaluateitonapreviouslyunknowncostfunctionft.Sincethenatureofthesequenceisunknowninadvance,weevaluateouralgorithmusingtheregret,thatisthesumofallthepreviousdifferencebetweentheonlinepredictionft(θt)andthebestﬁxedpointparameterft(θ∗)fromafeasiblesetXforalltheprevioussteps.Concretely,theregretisdeﬁnedas:R(T)=TXt=1[ft(θt)−ft(θ∗)](5)whereθ∗=argminθ∈XPTt=1ft(θ).WeshowAdamhasO(√T)regretboundandaproofisgivenintheappendix.Ourresultiscomparabletothebestknownboundforthisgeneralconvexonlinelearningproblem.Wealsousesomedeﬁnitionssimplifyournotation,wheregt,∇ft(θt)andgt,iastheithelement.Wedeﬁneg1:t,i∈Rtasavectorthatcontainstheithdimensionofthegradientsoveralliterationstillt,g1:t,i=[g1,i,g2,i,···,gt,i].Also,wedeﬁneγ,β21√β2.Ourfollowingtheoremholdswhenthelearningrateαtisdecayingatarateoft−12andﬁrstmomentrunningaveragecoefﬁcientβ1,tdecayexponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.Theorem4.1.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,kθm−θnk∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfyβ21√β2<1.Letαt=α√tandβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22α(1−β1)(1−λ)2OurTheorem4.1implieswhenthedatafeaturesaresparseandboundedgradients,thesum-mationtermcanbemuchsmallerthanitsupperboundPdi=1kg1:T,ik2<<dG∞√TandPdi=1pTbvT,i<<dG∞√T,inparticulariftheclassoffunctionanddatafeaturesareintheformofsection1.2in(Duchietal.,2011).TheirresultsfortheexpectedvalueE[Pdi=1kg1:T,ik2]alsoapplytoAdam.Inparticular,theadaptivemethod,suchasAdamandAdagrad,canachieveO(logd√T),animprovementoverO(√dT)forthenon-adaptivemethod.Decayingβ1,ttowardszeroisimpor-tantinourtheoreticalanalysisandalsomatchespreviousempiricalﬁndings,e.g.(Sutskeveretal.,2013)suggestsreducingthemomentumcoefﬁcientintheendoftrainingcanimproveconvergence.Finally,wecanshowtheaverageregretofAdamconverges,Corollary4.2.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,kθm−θnk∞≤D∞foranym,n∈{1,...,T}.Adamachievesthefollowingguarantee,forallT≥1.R(T)T=O(1√T)ThisresultcanbeobtainedbyusingTheorem4.1andPdi=1kg1:T,ik2≤dG∞√T.Thus,limT→∞R(T)T=0.5RELATEDWORKOptimizationmethodsbearingadirectrelationtoAdamareRMSProp(Tieleman&Hinton,2012;Graves,2013)andAdaGrad(Duchietal.,2011);theserelationshipsarediscussedbelow.OtherstochasticoptimizationmethodsincludevSGD(Schauletal.,2012),AdaDelta(Zeiler,2012)andthenaturalNewtonmethodfromRoux&Fitzgibbon(2010),allsettingstepsizesbyestimatingcurvature4PublishedasaconferencepaperatICLR2015fromﬁrst-orderinformation.TheSum-of-FunctionsOptimizer(SFO)(Sohl-Dicksteinetal.,2014)isaquasi-Newtonmethodbasedonminibatches,but(unlikeAdam)hasmemoryrequirementslinearinthenumberofminibatchpartitionsofadataset,whichisofteninfeasibleonmemory-constrainedsystemssuchasaGPU.Likenaturalgradientdescent(NGD)(Amari,1998),Adamemploysapreconditionerthatadaptstothegeometryofthedata,sincebvtisanapproximationtothediagonaloftheFisherinformationmatrix(Pascanu&Bengio,2013);however,Adam’spreconditioner(likeAdaGrad’s)ismoreconservativeinitsadaptionthanvanillaNGDbypreconditioningwiththesquarerootoftheinverseofthediagonalFisherinformationmatrixapproximation.RMSProp:AnoptimizationmethodcloselyrelatedtoAdamisRMSProp(Tieleman&Hinton,2012).Aversionwithmomentumhassometimesbeenused(Graves,2013).Thereareafewimpor-tantdifferencesbetweenRMSPropwithmomentumandAdam:RMSPropwithmomentumgener-atesitsparameterupdatesusingamomentumontherescaledgradient,whereasAdamupdatesaredirectlyestimatedusingarunningaverageofﬁrstandsecondmomentofthegradient.RMSPropalsolacksabias-correctionterm;thismattersmostincaseofavalueofβ2closeto1(requiredincaseofsparsegradients),sinceinthatcasenotcorrectingthebiasleadstoverylargestepsizesandoftendivergence,aswealsoempiricallydemonstrateinsection6.4.AdaGrad:AnalgorithmthatworkswellforsparsegradientsisAdaGrad(Duchietal.,2011).Itsbasicversionupdatesparametersasθt+1=θt−α·gt/qPti=1g2t.Notethatifwechooseβ2tobeinﬁnitesimallycloseto1frombelow,thenlimβ2→1bvt=t−1·Pti=1g2t.AdaGradcorrespondstoaversionofAdamwithβ1=0,inﬁnitesimal(1−β2)andareplacementofαbyanannealedversionαt=α·t−1/2,namelyθt−α·t−1/2·bmt/plimβ2→1bvt=θt−α·t−1/2·gt/qt−1·Pti=1g2t=θt−α·gt/qPti=1g2t.NotethatthisdirectcorrespondencebetweenAdamandAdagraddoesnotholdwhenremovingthebias-correctionterms;withoutbiascorrection,likeinRMSProp,aβ2inﬁnitesimallycloseto1wouldleadtoinﬁnitelylargebias,andinﬁnitelylargeparameterupdates.6EXPERIMENTSToempiricallyevaluatetheproposedmethod,weinvestigateddifferentpopularmachinelearningmodels,includinglogisticregression,multilayerfullyconnectedneuralnetworksanddeepconvolu-tionalneuralnetworks.Usinglargemodelsanddatasets,wedemonstrateAdamcanefﬁcientlysolvepracticaldeeplearningproblems.Weusethesameparameterinitializationwhencomparingdifferentoptimizationalgorithms.Thehyper-parameters,suchaslearningrateandmomentum,aresearchedoveradensegridandtheresultsarereportedusingthebesthyper-parametersetting.6.1EXPERIMENT:LOGISTICREGRESSIONWeevaluateourproposedmethodonL2-regularizedmulti-classlogisticregressionusingtheMNISTdataset.Logisticregressionhasawell-studiedconvexobjective,makingitsuitableforcomparisonofdifferentoptimizerswithoutworryingaboutlocalminimumissues.Thestepsizeαinourlogisticregressionexperimentsisadjustedby1/√tdecay,namelyαt=α√tthatmatcheswithourtheorat-icalpredictionfromsection4.Thelogisticregressionclassiﬁestheclasslabeldirectlyonthe784dimensionimagevectors.WecompareAdamtoacceleratedSGDwithNesterovmomentumandAdagradusingminibatchsizeof128.AccordingtoFigure1,wefoundthattheAdamyieldssimilarconvergenceasSGDwithmomentumandbothconvergefasterthanAdagrad.Asdiscussedin(Duchietal.,2011),Adagradcanefﬁcientlydealwithsparsefeaturesandgradi-entsasoneofitsmaintheoreticalresultswhereasSGDislowatlearningrarefeatures.Adamwith1/√tdecayonitsstepsizeshouldtheoraticallymatchtheperformanceofAdagrad.WeexaminethesparsefeatureproblemusingIMDBmoviereviewdatasetfrom(Maasetal.,2011).Wepre-processtheIMDBmoviereviewsintobag-of-words(BoW)featurevectorsincludingtheﬁrst10,000mostfrequentwords.The10,000dimensionBoWfeaturevectorforeachreviewishighlysparse.Assug-gestedin(Wang&Manning,2013),50%dropoutnoisecanbeappliedtotheBoWfeaturesduring5PublishedasaconferencepaperatICLR2015051015202530354045iterations over entire dataset0.20.30.40.50.60.7training costMNIST Logistic RegressionAdaGradSGDNesterovAdam020406080100120140160iterations over entire dataset0.200.250.300.350.400.450.50training costIMDB BoW feature Logistic RegressionAdagrad+dropoutRMSProp+dropoutSGDNesterov+dropoutAdam+dropoutFigure1:LogisticregressiontrainingnegativeloglikelihoodonMNISTimagesandIMDBmoviereviewswith10,000bag-of-words(BoW)featurevectors.trainingtopreventover-ﬁtting.Inﬁgure1,AdagradoutperformsSGDwithNesterovmomentumbyalargemarginbothwithandwithoutdropoutnoise.AdamconvergesasfastasAdagrad.TheempiricalperformanceofAdamisconsistentwithourtheoreticalﬁndingsinsections2and4.Sim-ilartoAdagrad,AdamcantakeadvantageofsparsefeaturesandobtainfasterconvergenceratethannormalSGDwithmomentum.6.2EXPERIMENT:MULTI-LAYERNEURALNETWORKSMulti-layerneuralnetworkarepowerfulmodelswithnon-convexobjectivefunctions.Althoughourconvergenceanalysisdoesnotapplytonon-convexproblems,weempiricallyfoundthatAdamoftenoutperformsothermethodsinsuchcases.Inourexperiments,wemademodelchoicesthatareconsistentwithpreviouspublicationsinthearea;aneuralnetworkmodelwithtwofullyconnectedhiddenlayerswith1000hiddenunitseachandReLUactivationareusedforthisexperimentwithminibatchsizeof128.First,westudydifferentoptimizersusingthestandarddeterministiccross-entropyobjectivefunc-tionwithL2weightdecayontheparameterstopreventover-ﬁtting.Thesum-of-functions(SFO)method(Sohl-Dicksteinetal.,2014)isarecentlyproposedquasi-Newtonmethodthatworkswithminibatchesofdataandhasshowngoodperformanceonoptimizationofmulti-layerneuralnet-works.WeusedtheirimplementationandcomparedwithAdamtotrainsuchmodels.Figure2showsthatAdammakesfasterprogressintermsofboththenumberofiterationsandwall-clocktime.Duetothecostofupdatingcurvatureinformation,SFOis5-10xslowerperiterationcom-paredtoAdam,andhasamemoryrequirementthatislinearinthenumberminibatches.Stochasticregularizationmethods,suchasdropout,areaneffectivewaytopreventover-ﬁttingandoftenusedinpracticeduetotheirsimplicity.SFOassumesdeterministicsubfunctions,andindeedfailedtoconvergeoncostfunctionswithstochasticregularization.WecomparetheeffectivenessofAdamtootherstochasticﬁrstordermethodsonmulti-layerneuralnetworkstrainedwithdropoutnoise.Figure2showsourresults;Adamshowsbetterconvergencethanothermethods.6.3EXPERIMENT:CONVOLUTIONALNEURALNETWORKSConvolutionalneuralnetworks(CNNs)withseverallayersofconvolution,poolingandnon-linearunitshaveshownconsiderablesuccessincomputervisiontasks.Unlikemostfullyconnectedneuralnets,weightsharinginCNNsresultsinvastlydifferentgradientsindifferentlayers.AsmallerlearningratefortheconvolutionlayersisoftenusedinpracticewhenapplyingSGD.WeshowtheeffectivenessofAdamindeepCNNs.OurCNNarchitecturehasthreealternatingstagesof5x5convolutionﬁltersand3x3maxpoolingwithstrideof2thatarefollowedbyafullyconnectedlayerof1000rectiﬁedlinearhiddenunits(ReLU’s).Theinputimagearepre-processedbywhitening,and6PublishedasaconferencepaperatICLR2015050100150200iterations over entire dataset10-210-1training costMNIST Multilayer Neural Network + dropoutAdaGradRMSPropSGDNesterovAdaDeltaAdam(a)(b)Figure2:TrainingofmultilayerneuralnetworksonMNISTimages.(a)Neuralnetworksusingdropoutstochasticregularization.(b)Neuralnetworkswithdeterministiccostfunction.Wecomparewiththesum-of-functions(SFO)optimizer(Sohl-Dicksteinetal.,2014)0.00.51.01.52.02.53.0iterations over entire dataset0.51.01.52.02.53.0training costCIFAR10 ConvNet First 3 EpochesAdaGradAdaGrad+dropoutSGDNesterovSGDNesterov+dropoutAdamAdam+dropout051015202530354045iterations over entire dataset10-410-310-210-1100101102training costCIFAR10 ConvNetAdaGradAdaGrad+dropoutSGDNesterovSGDNesterov+dropoutAdamAdam+dropoutFigure3:Convolutionalneuralnetworkstrainingcost.(left)Trainingcostfortheﬁrstthreeepochs.(right)Trainingcostover45epochs.CIFAR-10withc64-c64-c128-1000architecture.dropoutnoiseisappliedtotheinputlayerandfullyconnectedlayer.Theminibatchsizeisalsosetto128similartopreviousexperiments.Interestingly,althoughbothAdamandAdagradmakerapidprogressloweringthecostintheinitialstageofthetraining,showninFigure3(left),AdamandSGDeventuallyconvergeconsiderablyfasterthanAdagradforCNNsshowninFigure3(right).Wenoticethesecondmomentestimatebvtvanishestozerosafterafewepochsandisdominatedbythe(cid:15)inalgorithm1.ThesecondmomentestimateisthereforeapoorapproximationtothegeometryofthecostfunctioninCNNscomparingtofullyconnectednetworkfromSection6.2.Whereas,reducingtheminibatchvariancethroughtheﬁrstmomentismoreimportantinCNNsandcontributestothespeed-up.Asaresult,Adagradconvergesmuchslowerthanothersinthisparticularexperiment.ThoughAdamshowsmarginalimprovementoverSGDwithmomentum,itadaptslearningratescalefordifferentlayersinsteadofhandpickingmanuallyasinSGD.7PublishedasaconferencepaperatICLR2015β1=0β1=0.9β2=0.99β2=0.999β2=0.9999β2=0.99β2=0.999β2=0.9999(a) after 10 epochs(b) after 100 epochslog10(α)LossFigure4:Effectofbias-correctionterms(redline)versusnobiascorrectionterms(greenline)after10epochs(left)and100epochs(right)ontheloss(y-axes)whenlearningaVariationalAuto-Encoder(VAE)(Kingma&Welling,2013),fordifferentsettingsofstepsizeα(x-axes)andhyper-parametersβ1andβ2.6.4EXPERIMENT:BIAS-CORRECTIONTERMWealsoempiricallyevaluatetheeffectofthebiascorrectiontermsexplainedinsections2and3.Discussedinsection5,removalofthebiascorrectiontermsresultsinaversionofRMSProp(Tiele-man&Hinton,2012)withmomentum.Wevarytheβ1andβ2whentrainingavariationalauto-encoder(VAE)withthesamearchitectureasin(Kingma&Welling,2013)withasinglehiddenlayerwith500hiddenunitswithsoftplusnonlinearitiesanda50-dimensionalsphericalGaussianlatentvariable.Weiteratedoverabroadrangeofhyper-parameterchoices,i.e.β1∈[0,0.9]andβ2∈[0.99,0.999,0.9999],andlog10(α)∈[−5,...,−1].Valuesofβ2closeto1,requiredforrobust-nesstosparsegradients,resultsinlargerinitializationbias;thereforeweexpectthebiascorrectiontermisimportantinsuchcasesofslowdecay,preventinganadverseeffectonoptimization.InFigure4,valuesβ2closeto1indeedleadtoinstabilitiesintrainingwhennobiascorrectiontermwaspresent,especiallyatﬁrstfewepochsofthetraining.Thebestresultswereachievedwithsmallvaluesof(1−β2)andbiascorrection;thiswasmoreapparenttowardstheendofoptimizationwhengradientstendstobecomesparserashiddenunitsspecializetospeciﬁcpatterns.Insummary,AdamperformedequalorbetterthanRMSProp,regardlessofhyper-parametersetting.7EXTENSIONS7.1ADAMAXInAdam,theupdateruleforindividualweightsistoscaletheirgradientsinverselyproportionaltoa(scaled)L2normoftheirindividualcurrentandpastgradients.WecangeneralizetheL2normbasedupdateruletoaLpnormbasedupdaterule.Suchvariantsbecomenumericallyunstableforlargep.However,inthespecialcasewhereweletp→∞,asurprisinglysimpleandstablealgorithmemerges;seealgorithm2.We’llnowderivethealgorithm.Let,incaseoftheLpnorm,thestepsizeattimetbeinverselyproportionaltov1/pt,where:vt=βp2vt−1+(1−βp2)|gt|p(6)=(1−βp2)tXi=1βp(t−i)2·|gi|p(7)8PublishedasaconferencepaperatICLR2015Algorithm2:AdaMax,avariantofAdambasedontheinﬁnitynorm.Seesection7.1fordetails.Gooddefaultsettingsforthetestedmachinelearningproblemsareα=0.002,β1=0.9andβ2=0.999.Withβt1wedenoteβ1tothepowert.Here,(α/(1−βt1))isthelearningratewiththebias-correctiontermfortheﬁrstmoment.Alloperationsonvectorsareelement-wise.Require:α:StepsizeRequire:β1,β2∈[0,1):ExponentialdecayratesRequire:f(θ):StochasticobjectivefunctionwithparametersθRequire:θ0:Initialparametervectorm0←0(Initialize1stmomentvector)u0←0(Initializetheexponentiallyweightedinﬁnitynorm)t←0(Initializetimestep)whileθtnotconvergeddot←t+1gt←∇θft(θt−1)(Getgradientsw.r.t.stochasticobjectiveattimestept)mt←β1·mt−1+(1−β1)·gt(Updatebiasedﬁrstmomentestimate)ut←max(β2·ut−1,|gt|)(Updatetheexponentiallyweightedinﬁnitynorm)θt←θt−1−(α/(1−βt1))·mt/ut(Updateparameters)endwhilereturnθt(Resultingparameters)Notethatthedecaytermishereequivalentlyparameterisedasβp2insteadofβ2.Nowletp→∞,anddeﬁneut=limp→∞(vt)1/p,then:ut=limp→∞(vt)1/p=limp→∞ (1−βp2)tXi=1βp(t−i)2·|gi|p!1/p(8)=limp→∞(1−βp2)1/p tXi=1βp(t−i)2·|gi|p!1/p(9)=limp→∞ tXi=1(cid:16)β(t−i)2·|gi|(cid:17)p!1/p(10)=max(cid:0)βt−12|g1|,βt−22|g2|,...,β2|gt−1|,|gt|(cid:1)(11)Whichcorrespondstotheremarkablysimplerecursiveformula:ut=max(β2·ut−1,|gt|)(12)withinitialvalueu0=0.Notethat,convenientlyenough,wedon’tneedtocorrectforinitializationbiasinthiscase.AlsonotethatthemagnitudeofparameterupdateshasasimplerboundwithAdaMaxthanAdam,namely:|∆t|≤α.7.2TEMPORALAVERAGINGSincethelastiterateisnoisyduetostochasticapproximation,bettergeneralizationperformanceisoftenachievedbyaveraging.PreviouslyinMoulines&Bach(2011),Polyak-Ruppertaveraging(Polyak&Juditsky,1992;Ruppert,1988)hasbeenshowntoimprovetheconvergenceofstandardSGD,where¯θt=1tPnk=1θk.Alternatively,anexponentialmovingaverageovertheparameterscanbeused,givinghigherweighttomorerecentparametervalues.Thiscanbetriviallyimplementedbyaddingonelinetotheinnerloopofalgorithms1and2:¯θt←β2·¯θt−1+(1−β2)θt,with¯θ0=0.Initalizationbiascanagainbecorrectedbytheestimatorbθt=¯θt/(1−βt2).8CONCLUSIONWehaveintroducedasimpleandcomputationallyefﬁcientalgorithmforgradient-basedoptimiza-tionofstochasticobjectivefunctions.Ourmethodisaimedtowardsmachinelearningproblemswith9PublishedasaconferencepaperatICLR2015largedatasetsand/orhigh-dimensionalparameterspaces.Themethodcombinestheadvantagesoftworecentlypopularoptimizationmethods:theabilityofAdaGradtodealwithsparsegradients,andtheabilityofRMSProptodealwithnon-stationaryobjectives.Themethodisstraightforwardtoimplementandrequireslittlememory.Theexperimentsconﬁrmtheanalysisontherateofcon-vergenceinconvexproblems.Overall,wefoundAdamtoberobustandwell-suitedtoawiderangeofnon-convexoptimizationproblemsintheﬁeldmachinelearning.9ACKNOWLEDGMENTSThispaperwouldprobablynothaveexistedwithoutthesupportofGoogleDeepmind.WewouldliketogivespecialthankstoIvoDanihelka,andTomSchaulforcoiningthenameAdam.ThankstoKaiFanfromDukeUniversityforspottinganerrorintheoriginalAdaMaxderivation.ExperimentsinthisworkwerepartlycarriedoutontheDutchnationale-infrastructurewiththesupportofSURFFoundation.DiederikKingmaissupportedbytheGoogleEuropeanDoctorateFellowshipinDeepLearning.REFERENCESAmari,Shun-Ichi.Naturalgradientworksefﬁcientlyinlearning.Neuralcomputation,10(2):251–276,1998.Deng,Li,Li,Jinyu,Huang,Jui-Ting,Yao,Kaisheng,Yu,Dong,Seide,Frank,Seltzer,Michael,Zweig,Geoff,He,Xiaodong,Williams,Jason,etal.Recentadvancesindeeplearningforspeechresearchatmicrosoft.ICASSP2013,2013.Duchi,John,Hazan,Elad,andSinger,Yoram.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.TheJournalofMachineLearningResearch,12:2121–2159,2011.Graves,Alex.Generatingsequenceswithrecurrentneuralnetworks.arXivpreprintarXiv:1308.0850,2013.Graves,Alex,Mohamed,Abdel-rahman,andHinton,Geoffrey.Speechrecognitionwithdeeprecurrentneuralnetworks.InAcoustics,SpeechandSignalProcessing(ICASSP),2013IEEEInternationalConferenceon,pp.6645–6649.IEEE,2013.Hinton,G.E.andSalakhutdinov,R.R.Reducingthedimensionalityofdatawithneuralnetworks.Science,313(5786):504–507,2006.Hinton,Geoffrey,Deng,Li,Yu,Dong,Dahl,GeorgeE,Mohamed,Abdel-rahman,Jaitly,Navdeep,Senior,Andrew,Vanhoucke,Vincent,Nguyen,Patrick,Sainath,TaraN,etal.Deepneuralnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearchgroups.SignalProcessingMagazine,IEEE,29(6):82–97,2012a.Hinton,GeoffreyE,Srivastava,Nitish,Krizhevsky,Alex,Sutskever,Ilya,andSalakhutdinov,RuslanR.Im-provingneuralnetworksbypreventingco-adaptationoffeaturedetectors.arXivpreprintarXiv:1207.0580,2012b.Kingma,DiederikPandWelling,Max.Auto-EncodingVariationalBayes.InThe2ndInternationalConferenceonLearningRepresentations(ICLR),2013.Krizhevsky,Alex,Sutskever,Ilya,andHinton,GeoffreyE.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012.Maas,AndrewL,Daly,RaymondE,Pham,PeterT,Huang,Dan,Ng,AndrewY,andPotts,Christopher.Learningwordvectorsforsentimentanalysis.InProceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies-Volume1,pp.142–150.AssociationforComputationalLinguistics,2011.Moulines,EricandBach,FrancisR.Non-asymptoticanalysisofstochasticapproximationalgorithmsformachinelearning.InAdvancesinNeuralInformationProcessingSystems,pp.451–459,2011.Pascanu,RazvanandBengio,Yoshua.Revisitingnaturalgradientfordeepnetworks.arXivpreprintarXiv:1301.3584,2013.Polyak,BorisTandJuditsky,AnatoliB.Accelerationofstochasticapproximationbyaveraging.SIAMJournalonControlandOptimization,30(4):838–855,1992.10PublishedasaconferencepaperatICLR2015Roux,NicolasLandFitzgibbon,AndrewW.Afastnaturalnewtonmethod.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.623–630,2010.Ruppert,David.Efﬁcientestimationsfromaslowlyconvergentrobbins-monroprocess.Technicalreport,CornellUniversityOperationsResearchandIndustrialEngineering,1988.Schaul,Tom,Zhang,Sixin,andLeCun,Yann.Nomorepeskylearningrates.arXivpreprintarXiv:1206.1106,2012.Sohl-Dickstein,Jascha,Poole,Ben,andGanguli,Surya.Fastlarge-scaleoptimizationbyunifyingstochas-ticgradientandquasi-newtonmethods.InProceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),pp.604–612,2014.Sutskever,Ilya,Martens,James,Dahl,George,andHinton,Geoffrey.Ontheimportanceofinitializationandmomentumindeeplearning.InProceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),pp.1139–1147,2013.Tieleman,T.andHinton,G.Lecture6.5-RMSProp,COURSERA:NeuralNetworksforMachineLearning.Technicalreport,2012.Wang,SidaandManning,Christopher.Fastdropouttraining.InProceedingsofthe30thInternationalConfer-enceonMachineLearning(ICML-13),pp.118–126,2013.Zeiler,MatthewD.Adadelta:Anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701,2012.Zinkevich,Martin.Onlineconvexprogrammingandgeneralizedinﬁnitesimalgradientascent.2003.11PublishedasaconferencepaperatICLR201510APPENDIX10.1CONVERGENCEPROOFDeﬁnition10.1.Afunctionf:Rd→Risconvexifforallx,y∈Rd,forallλ∈[0,1],λf(x)+(1−λ)f(y)≥f(λx+(1−λ)y)Also,noticethataconvexfunctioncanbelowerboundedbyahyperplaneatitstangent.Lemma10.2.Ifafunctionf:Rd→Risconvex,thenforallx,y∈Rd,f(y)≥f(x)+∇f(x)T(y−x)TheabovelemmacanbeusedtoupperboundtheregretandourproofforthemaintheoremisconstructedbysubstitutingthehyperplanewiththeAdamupdaterules.Thefollowingtwolemmasareusedtosupportourmaintheorem.Wealsousesomedeﬁnitionssim-plifyournotation,wheregt,∇ft(θt)andgt,iastheithelement.Wedeﬁneg1:t,i∈Rtasavectorthatcontainstheithdimensionofthegradientsoveralliterationstillt,g1:t,i=[g1,i,g2,i,···,gt,i]Lemma10.3.Letgt=∇ft(θt)andg1:tbedeﬁnedasaboveandbounded,kgtk2≤G,kgtk∞≤G∞.Then,TXt=1sg2t,it≤2G∞kg1:T,ik2Proof.WewillprovetheinequalityusinginductionoverT.ThebasecaseforT=1,wehaveqg21,i≤2G∞kg1,ik2.Fortheinductivestep,TXt=1sg2t,it=T−1Xt=1sg2t,it+sg2T,iT≤2G∞kg1:T−1,ik2+sg2T,iT=2G∞qkg1:T,ik22−g2T+sg2T,iTFrom,kg1:T,ik22−g2T,i+g4T,i4kg1:T,ik22≥kg1:T,ik22−g2T,i,wecantakesquarerootofbothsideandhave,qkg1:T,ik22−g2T,i≤kg1:T,ik2−g2T,i2kg1:T,ik2≤kg1:T,ik2−g2T,i2pTG2∞Rearrangetheinequalityandsubstitutetheqkg1:T,ik22−g2T,iterm,G∞qkg1:T,ik22−g2T+sg2T,iT≤2G∞kg1:T,ik212PublishedasaconferencepaperatICLR2015Lemma10.4.Letγ,β21√β2.Forβ1,β2∈[0,1)thatsatisfyβ21√β2<1andboundedgt,kgtk2≤G,kgtk∞≤G∞,thefollowinginequalityholdsTXt=1bm2t,iptbvt,i≤21−γ1√1−β2kg1:T,ik2Proof.Undertheassumption,√1−βt2(1−βt1)2≤1(1−β1)2.WecanexpandthelastterminthesummationusingtheupdaterulesinAlgorithm1,TXt=1bm2t,iptbvt,i=T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2(PTk=1(1−β1)βT−k1gk,i)2qTPTj=1(1−β2)βT−j2g2j,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2TXk=1T((1−β1)βT−k1gk,i)2qTPTj=1(1−β2)βT−j2g2j,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2TXk=1T((1−β1)βT−k1gk,i)2qT(1−β2)βT−k2g2k,i≤T−1Xt=1bm2t,iptbvt,i+p1−βT2(1−βT1)2(1−β1)2pT(1−β2)TXk=1T(cid:18)β21√β2(cid:19)T−kkgk,ik2≤T−1Xt=1bm2t,iptbvt,i+TpT(1−β2)TXk=1γT−kkgk,ik2Similarly,wecanupperboundtherestofthetermsinthesummation.TXt=1bm2t,iptbvt,i≤TXt=1kgt,ik2pt(1−β2)T−tXj=0tγj≤TXt=1kgt,ik2pt(1−β2)TXj=0tγjForγ<1,usingtheupperboundonthearithmetic-geometricseries,Pttγt<1(1−γ)2:TXt=1kgt,ik2pt(1−β2)TXj=0tγj≤1(1−γ)2√1−β2TXt=1kgt,ik2√tApplyLemma10.3,TXt=1bm2t,iptbvt,i≤2G∞(1−γ)2√1−β2kg1:T,ik2Tosimplifythenotation,wedeﬁneγ,β21√β2.Intuitively,ourfollowingtheoremholdswhenthelearningrateαtisdecayingatarateoft−12andﬁrstmomentrunningaveragecoefﬁcientβ1,tdecayexponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.Theorem10.5.Assumethatthefunctionfthasboundedgradients,k∇ft(θ)k2≤G,k∇ft(θ)k∞≤G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,kθn−θmk2≤D,13PublishedasaconferencepaperatICLR2015kθm−θnk∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfyβ21√β2<1.Letαt=α√tandβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(β1+1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22α(1−β1)(1−λ)2Proof.UsingLemma10.2,wehave,ft(θt)−ft(θ∗)≤gTt(θt−θ∗)=dXi=1gt,i(θt,i−θ∗,i)Fromtheupdaterulespresentedinalgorithm1,θt+1=θt−αtbmt/pbvt=θt−αt1−βt1(cid:18)β1,t√bvtmt−1+(1−β1,t)√bvtgt(cid:19)Wefocusontheithdimensionoftheparametervectorθt∈Rd.Subtractthescalarθ∗,iandsquarebothsidesoftheaboveupdaterule,wehave,(θt+1,i−θ∗,i)2=(θt,i−θ∗,i)2−2αt1−βt1(β1,tpbvt,imt−1,i+(1−β1,t)pbvt,igt,i)(θt,i−θ∗,i)+α2t(bmt,ipbvt,i)2WecanrearrangetheaboveequationanduseYoung’sinequality,ab≤a2/2+b2/2.Also,itcanbeshownthatpbvt,i=qPtj=1(1−β2)βt−j2g2j,i/p1−βt2≤kg1:t,ik2andβ1,t≤β1.Thengt,i(θt,i−θ∗,i)=(1−βt1)pbvt,i2αt(1−β1,t)(cid:18)(θt,i−θ∗,t)2−(θt+1,i−θ∗,i)2(cid:19)+β1,t(1−β1,t)bv14t−1,i√αt−1(θ∗,i−θt,i)√αt−1mt−1,ibv14t−1,i+αt(1−βt1)pbvt,i2(1−β1,t)(bmt,ipbvt,i)2≤12αt(1−β1)(cid:18)(θt,i−θ∗,t)2−(θt+1,i−θ∗,i)2(cid:19)pbvt,i+β1,t2αt−1(1−β1,t)(θ∗,i−θt,i)2pbvt−1,i+β1αt−12(1−β1)m2t−1,ipbvt−1,i+αt2(1−β1)bm2t,ipbvt,iWeapplyLemma10.4totheaboveinequalityandderivetheregretboundbysummingacrossallthedimensionsfori∈1,...,dintheupperboundofft(θt)−ft(θ∗)andthesequenceofconvexfunctionsfort∈1,...,T:R(T)≤dXi=112α1(1−β1)(θ1,i−θ∗,i)2pbv1,i+dXi=1TXt=212(1−β1)(θt,i−θ∗,i)2(pbvt,iαt−pbvt−1,iαt−1)+β1αG∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+αG∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1TXt=1β1,t2αt(1−β1,t)(θ∗,i−θt,i)2pbvt,i14PublishedasaconferencepaperatICLR2015Fromtheassumption,kθt−θ∗k2≤D,kθm−θnk∞≤D∞,wehave:R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+D2∞2αdXi=1tXt=1β1,t(1−β1,t)ptbvt,i≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+D2∞G∞√1−β22αdXi=1tXt=1β1,t(1−β1,t)√tWecanusearithmeticgeometricseriesupperboundforthelastterm:tXt=1β1,t(1−β1,t)√t≤tXt=11(1−β1)λt−1√t≤tXt=11(1−β1)λt−1t≤1(1−β1)(1−λ)2Therefore,wehavethefollowingregretbound:R(T)≤D22α(1−β1)dXi=1pTbvT,i+α(1+β1)G∞(1−β1)√1−β2(1−γ)2dXi=1kg1:T,ik2+dXi=1D2∞G∞√1−β22αβ1(1−λ)215